# Your anytime anywhere deployable - RAG Assistant is HERE

Hello everyone 
Welcome to this project — a fully local **Retrieval-Augmented Generation (RAG) Assistant** designed for the Retrieval of **Course/Project**.

This system can **understand your question**, find **exact video segments** (video number + timestamps), and answer using a **local LLM** running on your machine.  
Everything is done using a **self-built pipeline** involving Whisper, ffmpeg, Cosine Similarity, Embeddings, and Ollama.

---

## Features

- Automatically extracts audio from lecture videos  
- Whisper-based transcription (Hindi → English translation)  
- Subtitle chunk creation & merging  
- High-quality embeddings using **bge-m3** (via Ollama)  
- Semantic search using **cosine similarity**  
- Llama 3.2 powered answer generation  
- Complete offline pipeline — *no API keys needed*  
- Points you to the **correct video + timestamp** where the concept is explained  

---

## To begin , you need to install all the necessary dependencies:

Install required dependencies using:

     pip install -r requirements.txt

Then pull the required models in Ollama :

    ollama pull bge-m3

---

## Here's how you can run

Follow these steps exactly:

## 1)Install all the required dependencies

Install all the dependencies using the command:
```bash
pip install -r requirements.txt
```

## 2)Pull the model

Pull the ollama model using the command:
```bash
ollama pull bge-m3
```

## 3)Run main.py file

This runs the entire script in one click.
You can run it using:
```bash
python main.py
```
---

## Folder Structure - IMPORTANT NOTE

This project contains several folders that are **intentionally empty in the GitHub repository**.  
They are automatically filled as you run the pipeline.

**Here is what each folder represents:**

```bash
0) Create "video/" folder

Before running the code you need to create "video/" folder in your directory.
```

```bash
1) video/

Put your ".mp4" course videos here.
This is the only folder where you add files manually.  
All other folders below are auto-generated by the pipeline.
```

```bash
2) final_videos/

Automatically gets created by: "videos_processing.py"  
This folder stores renamed and cleaned video files for consistent processing.
```

```bash
3) final_audios/

Automatically gets created by: "videos_processing.py" 
This contains extracted .mp3 audio files from each video.
```

```bash
4) new_jsons/

Automatically gets created by: `mp3_to_json.py`  
This folder stores raw Whisper transcription JSON (Hindi → English translated subtitles).
```

```bash
5) final_jsons/

Automatically gets created by: `merge_chunks.py`  
This folder holds the **processed & merged subtitle chunks** that improve retrieval quality.
```

---

## Final Reminder for users

***Before running the pipeline***:

1. Place your `.mp4` videos inside the `video/` folder  
2. Run the scripts step-by-step  
3. All other folders will be auto-generated and populated

---
##  Here's how I have used this RAG ASSISTANT

   *I have stored all my raw, un-cleaned videos in `video` folder*
<img width="1428" height="477" alt="video" src="https://github.com/user-attachments/assets/2377d527-438b-4040-a5e6-3e10e5a20a1b" />


   *This is how my `final_videos` folder look like after running `videos_processing.py` file*

<img width="1331" height="460" alt="final_videos" src="https://github.com/user-attachments/assets/0923df1f-c6f8-4924-9f10-0a8b7f1ac532" />

   *This is how my `final_audios` folder looks like after using `ffmpeg` to convert all my videos to `.mp3` file for transcribing*

<img width="940" height="447" alt="final_audios" src="https://github.com/user-attachments/assets/9e3bd482-2e9f-465d-b3bd-c8421ce6e725" />


   *This is how smaller chunks in `new_jsons` folder look created using `mp2_to_json.py`*
<img width="1251" height="863" alt="smaller_chunks_(new_jsons)" src="https://github.com/user-attachments/assets/ea561346-7664-4fce-8187-856557f617c9" />



   *Now, Combining all the smaller `.json` file chunks from `new_jsons` folder to bigger chunks in `final_jsons` folder using `merge_chunks.py`*

<img width="1667" height="829" alt="biggerchunks_from_(final_jsons)" src="https://github.com/user-attachments/assets/c78ceadc-3a31-4070-859f-53659ea24c39" />


   *Now, After preprocessing all `.jsons` file from `final_json` using `preprocess_json.py` , I have converted them to embeddings and stored them in `final_embeddings.joblib` which will help us to retrieve using `cosine_similarity`*

<img width="237" height="30" alt="image" src="https://github.com/user-attachments/assets/1e47056d-6847-41f7-a0c4-4783830d9d6e" />


   *Now , After executing `final_output.py` , I have asked the question to the llm and the response gets stored in `response.txt`*

Ask a question: Where is float and bar taught in this course . 
`Reponse`:
<img width="1447" height="331" alt="Screenshot 2025-11-28 113058" src="https://github.com/user-attachments/assets/3ec9e63b-fbac-4945-a620-3817906293a1" />


---
##  Project Structure
So , This is how I have structured the project :
```
├── videos_processing.py # Renames videos and extracts audio
├── mp3_to_json.py # Converts audio to whisper transcripts
├── merge_chunks.py # Used to merge small subtitle chunks
├── preprocess_jsons.py # Used to generate embeddings for each chunk
├── final_output.py # Question answering system
├── main.py # Runs the entire script without running each file separately
├── video/ # For adding raw .mp4 videos
├── final_videos/ # Gets created to store clean renamed videos
├── final_audios/ # Gets created for storing all the extracted .mp3 files
├── new_jsons/ #Includes all the whisper transcription outputs
├── final_jsons/ #All the merged subtitle chunks get stored here 
├── final_embeddings.joblib # Created Embeddings DataFrame gets stored here
├── prompt.txt #The prompt given by the user gets stored here
├── response.txt # It stored Model’s final answer
├── requirements.txt
└── README.md
```

---

## How this workflow works

Video to Audio extraction (ffmpeg)

Audio to Text conversion (Whisper medium)

Subtitle chunks to Merged chunks

Embeddings created using bge-m3

User query gets Embedded & matched using cosine similarity

Top 3 relevant text chunks are extracted

Llama 3.2 generates a human answer with timestamps


## Models Used

Whisper (medium) – transcription + Hindi → English translation

bge-m3 – embedding model (via Ollama)

Llama 3.2 – LLM for final answer generation

## Example Use Case
```bash
Ask a question: "Where is float & clear taught in this course"
```
RESULT:

```bash
Hello! I see you're asking about where float and clear are taught in this course. The relevant content is actually covered in Video 34: "CSS Float & Clear".

The explanation of how float and clear work starts around 35.4 seconds into that video, and continues until 62.38 seconds. You can easily find those timestamps on the video.

If you want to get a better understanding of both concepts, the instructor also explains that if you're using both float and clear, you'll need to use "clear both" to avoid any overlap or other issues, starting at around 397.8-407.8 seconds into Video 34.

So, I'd recommend checking out those timestamps in Video 34: "CSS Float & Clear".
```
## Contact

For suggestions, improvements, or issues, feel free to open a GitHub issue.

# Your anytime anywhere deployable - RAG Assistant is HERE

Hello everyone 
Welcome to this project — a fully local **Retrieval-Augmented Generation (RAG) Assistant** designed for the Retrieval of **Course/Project**.

This system can **understand your question**, find **exact video segments** (video number + timestamps), and answer using a **local LLM** running on your machine.  
Everything is done using a **self-built pipeline** involving Whisper, ffmpeg, Cosine Similarity, Embeddings, and Ollama.

---

## Features

- Automatically extracts audio from lecture videos  
- Whisper-based transcription (Hindi → English translation)  
- Subtitle chunk creation & merging  
- High-quality embeddings using **bge-m3** (via Ollama)  
- Semantic search using **cosine similarity**  
- Llama 3.2 powered answer generation  
- Complete offline pipeline — *no API keys needed*  
- Points you to the **correct video + timestamp** where the concept is explained  

---
## Folder Structure - IMPORTANT NOTE

This project contains several folders that are **intentionally empty in the GitHub repository**.  
They are automatically filled as you run the pipeline.

**Here is what each folder represents:**

```bash
1) video/
    ├── .gitkeep

Put your ".mp4" course videos here.
This is the only folder where you add files manually.  
All other folders below are auto-generated by the pipeline.
```

---

```bash
2) final_videos/
    ├── .gitkeep

Created by: "01_videos_processing.py"  
This folder stores renamed and cleaned video files for consistent processing.
```

---

```bash
3) final_audios/
    ├── .gitkeep

Created by: "01_videos_processing.py" 
This contains extracted .mp3 audio files from each video.
```

---

```bash
4) new_jsons/
    ├── .gitkeep

Created by: `02_mp3_to_json.py`  
This folder stores raw Whisper transcription JSON (Hindi → English translated subtitles).
```

```bash
5) final_jsons/
    ├── .gitkeep

Created by: `03_merge_chunks.py`  
This folder holds the **processed & merged subtitle chunks** that improve retrieval quality.
```

---

## Here's why these folder are empty
```bash
GitHub does not track large or auto-generated files such as:

- Videos (`.mp4`)
- Audio files (`.mp3`)
- Whisper output (`.json`)
- Merged chunks (`.json`)
- Embeddings (`final_embeddings.joblib`)

These are ignored using `.gitignore` to prevent:

- Huge repository size  
- Duplicate copyrighted materials  
- Slow cloning  
- Exposing user-specific data
```
---
## Final Reminder for users

***Before running the pipeline***:

1. Place your `.mp4` videos inside the `video/` folder  
2. Run the scripts step-by-step  
3. All other folders will be auto-generated and populated

This keeps your repo clean, legal, and easy for others to use.

---
##  Here's how I have used this RAG ASSISTANT

   *I have stored all my raw, un-cleaned videos in `video` folder*
<img width="1428" height="477" alt="video" src="https://github.com/user-attachments/assets/2377d527-438b-4040-a5e6-3e10e5a20a1b" />


   *Then after cleaning the raw videos and storing them in `final_videos` folder using `01_videos_processing.py`*

<img width="1331" height="460" alt="final_videos" src="https://github.com/user-attachments/assets/0923df1f-c6f8-4924-9f10-0a8b7f1ac532" />

   *Now , after using `ffmpeg` to convert all my videos to `.mp3` file for transcribing , I have stored all .mp3 files in `final_audios` folder to convert them later to .json file*

<img width="940" height="447" alt="final_audios" src="https://github.com/user-attachments/assets/9e3bd482-2e9f-465d-b3bd-c8421ce6e725" />


   *This is how my smaller chunks in `new_jsons` folder look using `02_mp2_to_json.py`*
<img width="1251" height="863" alt="smaller_chunks_(new_jsons)" src="https://github.com/user-attachments/assets/ea561346-7664-4fce-8187-856557f617c9" />



   *Now, Combining all the smaller .json chunks from `new_jsons` folder to bigger chunks in `final_jsons` folder using `03_merge_chunks.py`*

<img width="1667" height="829" alt="biggerchunks_from_(final_jsons)" src="https://github.com/user-attachments/assets/c78ceadc-3a31-4070-859f-53659ea24c39" />


   *Now, After preprocessing all .jsons file from `final_json` using `04_preprocess_json.py` , I have converted them to embeddings and stored them in `final_embeddings.joblib` which will help us to retrieve using `cosine_similarity`*

<img width="237" height="30" alt="image" src="https://github.com/user-attachments/assets/1e47056d-6847-41f7-a0c4-4783830d9d6e" />


   *Now , After executing `05_final_output.py` , I have asked the question to the llm and the response gets stored in `response.txt`*

Ask a question: Where is float and bar taught in this course
Reponse:<img width="1447" height="331" alt="Screenshot 2025-11-28 113058" src="https://github.com/user-attachments/assets/3ec9e63b-fbac-4945-a620-3817906293a1" />


---
##  Project Structure
So , This is how I have structured the project :
```
├── 01_videos_processing.py # Rename videos and extracts audio
├── 02_mp3_to_json.py # Convert audio to whisper transcripts
├── 03_merge_chunks.py # Merge small subtitle chunks
├── 04_preprocess_jsons.py # Generate embeddings for each chunk
├── 05_final_output.py # Question answering system
├── video/ # raw .mp4 videos
├── final_videos/ # Clean renamed videos
├── final_audios/ # Extracted .mp3 files
├── new_jsons/ # Whisper transcription outputs
├── final_jsons/ # Merged subtitle chunks
├── final_embeddings.joblib # Embeddings DataFrame
├── prompt.txt # Saved prompt sent to Llama
├── response.txt # Model’s final answer
├── requirements.txt
└── README.md

To begin , you need to install all the necessary dependencies:

Install required dependencies using:

     pip install -r requirements.txt

You must install:
    ffmpeg
    Ollama

Then pull the required models in Ollama:
    ollama pull bge-m3
    ollama pull llama3.2

```  
## Here's how you can run

Follow these steps exactly:

## 1)Process raw videos

Renames messy video titles and extracts audio:
```bash
python 01_videos_processing.py
```

## 2)Convert audio to subtitles

Uses Whisper for transcription (Hindi → English):
```bash
python 02_mp3_to_json.py
```

## 3)Merge subtitle chunks

Creates cleaner, bigger chunks of text:
```bash
python 03_merge_chunks.py
```

## 4)Generate embeddings

Creates embeddings using bge-m3 and stores them:
```bash
python 04_preprocess_jsons.py
```
This generates:                
               final_embeddings.joblib

## 5)Run the Question-Answering System

Ask any question from your Web Development course:
```bash
python 05_final_output.py
```
You will get:

A human-like explanation

Video number  

Start & end timestamps

Direct reference to where the concept is taught

## How this workflow works

Video to Audio extraction (ffmpeg)

Audio to Text conversion (Whisper medium)

Subtitle chunks to Merged chunks

Embeddings created using bge-m3

User query gets Embedded & matched using cosine similarity

Top 3 relevant text chunks are extracted

Llama 3.2 generates a human answer with timestamps


## Models Used

Whisper (medium) – transcription + Hindi → English translation

bge-m3 – embedding model (via Ollama)

Llama 3.2 – LLM for final answer generation

## Example Use Case
```bash
Ask a question: "Where is float & clear taught in this course"
```
RESULT:

```bash
Hello! I see you're asking about where float and clear are taught in this course. The relevant content is actually covered in Video 34: "CSS Float & Clear".

The explanation of how float and clear work starts around 35.4 seconds into that video, and continues until 62.38 seconds. You can easily find those timestamps on the video.

If you want to get a better understanding of both concepts, the instructor also explains that if you're using both float and clear, you'll need to use "clear both" to avoid any overlap or other issues, starting at around 397.8-407.8 seconds into Video 34.

So, I'd recommend checking out those timestamps in Video 34: "CSS Float & Clear".
```
## Contact

For suggestions, improvements, or issues, feel free to open a GitHub issue.
